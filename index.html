<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Charm of Ancient Towers in Chongqing</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">InsightEdit: Towards Better Instruction Following for Image Editing</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Yingjing Xu<sup class="small-sup">1</sup>,<sup class="corresponding-author">*</sup>
            </span>,
            <span class="author-block">
              Jie Kong<sup class="small-sup">2</sup>,<sup class="corresponding-author">*‡</sup>
            </span>,
            <span class="author-block">
              Jiazhi Wang<sup class="small-sup">2</sup>,<sup class="corresponding-author"></sup>
            </span>
            <span class="author-block">
              Xiao Pan<sup class="small-sup">1</sup>,<sup class="corresponding-author"></sup>
            </span>
            <span class="author-block">
              Bo Lin<sup class="small-sup">1</sup>,<sup class="corresponding-author"></sup>
            </span>
            <span class="author-block">
              Qiang Liu<sup class="small-sup">2</sup>,<sup class="corresponding-author"></sup>
            </span>
            
          </div>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup class="small-sup">1</sup>Zhejiang University,</span>
            <span class="author-block"><sup class="small-sup">2</sup>01.ai</span>
          </div>
          
          <div class="footnotes">
            <sup class="small-sup">*</sup>Co-first authors.
            <sup class="corresponding-author">‡</sup>Corresponding authors.
          </div>
          <!-- <div class="publication-authors">
            <span class="author-block">Yingjing Xu, Zhejiang University, School of Software, Ningbo, China</span>
            <span class="author-block">Xueyan Cai, Zhejiang University, Hangzhou, Zhejiang/China, China</span>
            <span class="author-block">Zihong Zhou, Zhejiang University, Hangzhou, China</span>
            <span class="author-block">Mengru Xue, Ningbo Research Institute, Zhejiang University, Ningbo, Ningbo, Zhejiang, China, mengruxue@zju.edu.cn</span>
            <span class="author-block">Bo Wang, Department of Neurology, the Second Affiliated Hospital, Zhejiang University, Hangzhou, China</span>
            <span class="author-block">Haotian Wang, Zhejiang University, Hangzhou, China, wanght816@zju.edu.cn</span>
            <span class="author-block">Zhengke Li, Zhejiang University, Hangzhou, China</span>
            <span class="author-block">Chentian Weng, Zhejiang University, Hangzhou, China, 22351214@zju.edu.cn</span>
            <span class="author-block">Xiaofeng Ma, Zhejiang University, Hangzhou, China</span>
            <span class="author-block">Wei Luo, Computer Science and Technology, Zhejiang University, Hangzhou, China, yaoch@zju.edu.cn</span>
            <span class="author-block">Cheng Yao, Computer Science and Technology, Zhejiang University, Hangzhou, China, yaoch@zju.edu.cn</span>
            <span class="author-block">Bo Lin, School of Software Technology, Zhejiang University, Ningbo, China Innovation Centre for Information, Binjiang Institute of Zhejiang University, Hangzhou, China</span>
          </div> -->

          

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<div class="column has-text-centered">
  <div class="publication-links">
    <!-- PDF Link. -->
    <!-- <span class="link-block">
      <a href="https://arxiv.org/pdf/2011.12948"
         class="external-link button is-normal is-rounded is-dark">
        <span class="icon">
            <i class="fas fa-file-pdf"></i>
        </span>
        <span>Paper</span>
      </a>
    </span> -->
    <span class="link-block">
      <a href=""
         class="external-link button is-normal is-rounded is-dark">
        <span class="icon">
            <i class="ai ai-arxiv"></i>
        </span>
        <span>arXiv</span>
      </a>
    </span>
    <!-- Video Link. -->
    <!-- <span class="link-block">
      <a href="https://www.bilibili.com/video/BV1jk4y1p7Kn"
         class="external-link button is-normal is-rounded is-dark">
        <span class="icon">
            <i class="fab fa-youtube"></i>
        </span>
        <span>Video</span>
      </a>
    </span> -->
    <!-- Code Link. -->
    <span class="link-block">
      <a href="https://github.com/poppyxu/InsightEdit"
         class="external-link button is-normal is-rounded is-dark">
        <span class="icon">
            <i class="fab fa-github"></i>
        </span>
        <span>Code</span>
        </a>
    </span>
    <!-- Dataset Link. -->
    <!-- <span class="link-block">
      <a href="https://github.com/google/nerfies/releases/tag/0.1"
         class="external-link button is-normal is-rounded is-dark">
        <span class="icon">
            <i class="far fa-images"></i>
        </span>
        <span>Data</span>
        </a> 
      </span> -->
  </div>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" src="./static/images/showcase.jpg" alt="showcase" style="display: block; margin-left: auto; margin-right: auto; width: 85%; height: auto;">      <h2 class="subtitle has-text-centered">
      </h2>
    </div>
    <div>
      We propose InsightEdit, an end-to-end instruction-based image editing model, trained on high-quality data and designed to fully harness the capabilities of Multimodal Large Language Models (MLLM), achieving high-quality edits with strong instruction-following and background consistency.
    </div>

  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section> -->


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Hypomimia is a non-motor symptom of Parkinson's disease that manifests as delayed facial movements and expressions, along with challenges in articulation and emotion. Currently, subjective evaluation by neurologists is the primary method for hypomimia detection, and conventional rehabilitation approaches heavily rely on  verbal prompts from rehabilitation physicians. There remains a deficiency in accessible, user-friendly and scientifically rigorous assistive tools for hypomimia treatments. To address this, we introduce HypomimaCoach, an Action Unit (AU)-based digital therapy system for hypomimia detection and rehabilitation in Parkinson's disease. We employ Graph neural network (GNN) for hypomimia detection, modeling facial relationships based on AU features. For rehabilitation, we design a set of rehabilitation trainings based on AUs and provide patients with real-time feedbacks through additional AU recognization model, which guide them through their training routines. User study with seven PD patients and ten physicians confirme the system's usability, effectiveness, and acceptability, receiving positive feedbacks.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Animation. -->

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Abstract</h2>

        <!-- Interpolating. -->
        <!-- <h3 class="title is-4">Interpolating states</h3> -->
        <div class="content has-text-justified">
          <p>
            In this paper, we focus on the task of instruction-based image editing. Previous works like InstructPix2Pix, InstructDiffusion, and SmartEdit have explored end-to-end editing.
However, two limitations still remain: First, existing datasets suffer from low resolution, poor background consistency, and overly simplistic instructions. Second, current approaches mainly condition on the text while the rich image information is underexplored, therefore inferior in complex instruction following and maintaining background consistency. Targeting these issues, we first curated the AdvancedEdit dataset using a novel data construction pipeline, formulating a large-scale dataset with high visual quality, complex instructions, and good background consistency. 
Then, to further inject the rich image information, we introduce a two-stream bridging mechanism utilizing both the textual and visual features reasoned by the powerful Multimodal Large Language Models (MLLM) to guide the image editing process more precisely. 
Extensive results demonstrate that our approach, InsightEdit, achieves state-of-the-art performance, excelling in complex instruction following and maintaining high background consistency with the original image.          </p>
          </p>
  
        </div>

        <br/>

        <!-- <div class="content has-text-centered">
          <img id="replay-image" src="./static/images/pipeline.png" alt="Hypomimia Detection" style="width:75%; height:auto;">
        </div> -->
        <!--/ Re-rendering. -->

      </div>

    
    </div>


    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Data Construction</h2>

        <!-- Interpolating. -->
  
        <div class="content has-text-justified">
          <p>
            We propose an automated data construction pipeline fo-160
cused on generating high-fidelity, fine-grained image-161
editing pairs with detailed instructions that demonstrate ad-162
vanced reasoning and understanding. We categorize the im-163
age editing tasks into three types: removal, addition, and164
replacement. 
          </p>
        </div>
        <div class="content has-text-centered">
          <div style="display: flex; justify-content: space-between;">
            <img src="./static/images/data_construction.jpg" alt="Data Construction" style="display: block; margin-left: auto; margin-right: auto; width: 75%; height: auto;">
          </div>
          <div>
            The overall data construction pipeline.
            (1) Captioning: Object Extraction: Utilizing VLM to generate a global caption from the source image, and further get an object JSON list contains both simple caption and detailed caption. 
            (2) Mask Generation: Utilizing GroundedSAM to obtain the corresponding mask of each object. 
            (3) Editing Pair Construction: Utilizing mask-based image editing model to construct target image and templated instruction. 
            (4) Instruction Recaptioning: Utilizing VLM to rewrite instruction to gain diverse instructions. 
            (5) Quality Evaluation: Filtering the datasets using VIEScore.
          </div>
        </div>

        <br/>
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <!--/ Re-rendering. -->

      </div>
    </div>
    <!--/ Animation. -->

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Method</h2>

        <!-- Interpolating. -->
  
        <div class="content has-text-justified">
          <p>
            The overall architecture of InsightEdit is depicted in Figure. It mainly consists of a comprehension module, a bridging module, and a generation module. Specifically, the comprehension module leverages MLLM to comprehend the image editing task; the bridging module integrates both text and image features into the denoising process of the diffusion model; and the generation module receives editing guidance via the diffusion model to generate the target image.          
          </p>

          </div>
          <div class="content has-text-centered">
            <div style="display: flex; justify-content: space-between;">
              <img src="./static/images/method.jpg" alt="Method" style="display: block; margin-left: auto; margin-right: auto; width: 75%; height: auto;">
            </div>
            <div>
              The overall architecture of InsightEdit. It mainly consists of three parts: (1) Comprehension Module: A comprehension module that leverages MLLM to perceive and comprehend the image editing task; (2) Bridging Module: A bridging module that better interacts and extracts both the textual and image features; (3) Generation Module: A generation module that receives editing guidance via diffusion model to generate the target image.
            </div>
          </div>

      </div>
    </div>


    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Results</h2>

        <!-- Interpolating. -->
        <div class="content has-text-justified">
          <p>
            The overall architecture of InsightEdit is depicted in Figure. It mainly consists of a comprehension module, a bridging module, and a generation module. Specifically, the comprehension module leverages MLLM to comprehend the image editing task; the bridging module integrates both text and image features into the denoising process of the diffusion model; and the generation module receives editing guidance via the diffusion model to generate the target image.          
          </p>

          </div>
          <div class="content has-text-centered">
            <div style="display: flex; justify-content: space-between;">
              <img src="./static/images/result.jpg" alt="Qualitative Result" style="display: block; margin-left: auto; margin-right: auto; width: 75%; height: auto;">
            </div>
            <div>
              Qualitative comparison on AdvancedEdit. InsightEdit shows superior instruction following and background consistency capability.            </div>
          </div>

      </div>
    </div>


        <br/>
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <!--/ Re-rendering. -->

      </div>
    </div>
    <!-- Concurrent Work. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Concurrent Work. -->

  </div>
</section>

<!-- 
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a> -->
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
